{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning \n",
    "Suppose you ask a complex question to thousands of random people, then aggregate their answers. In\n",
    "many cases you will find that this aggregated answer is better than an expert’s answer. This is called the\n",
    "wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as\n",
    "classifiers or regressors), you will often get better predictions than with the best individual predictor. A\n",
    "group of predictors is called an ensemble; thus, this technique is called Ensemble Learning, and an\n",
    "Ensemble Learning algorithm is called an Ensemble method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, you can train a group of **Decision Tree classifiers**, each on a **different random subset** of the\n",
    "training set. To make predictions, you just obtain the predictions of all **individual trees**, then predict the\n",
    "class that gets the most votes. Such an ensemble of **Decision Trees** is called a **Random Forest**, and despite its simplicity, this is one of the most **powerful Machine Learning algorithms** available today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I- Voting Classifiers\n",
    "Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a\n",
    "Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors\n",
    "classifier, and perhaps a few more.\n",
    "\n",
    "A very simple way to create an even better classifier is to **aggregate the predictions** of each classifier and\n",
    "predict the class that gets the **most votes**. This majority-vote classifier is called a **hard voting classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VotingClassifiers Img](votingClassifier.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the best classifier in\n",
    "the ensemble. In fact, even if each classifier is a weak learner (meaning it does only slightly better than\n",
    "random guessing), the ensemble can still be a strong learner (achieving high accuracy), provided there\n",
    "are a sufficient number of weak learners and they are sufficiently diverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppose you build an ensemble containing 1,000 classifiers that are individually correct only\n",
    "51% of the time (barely better than random guessing). If you predict the majority voted class, you can\n",
    "hope for up to 75% accuracy! However, this is only true if all classifiers are perfectly independent,\n",
    "making uncorrelated errors, which is clearly not the case since they are trained on the same data. They are\n",
    "likely to make the same types of errors, so there will be many majority votes for the wrong class, reducing\n",
    "the ensemble’s accuracy.\n",
    "\n",
    "\n",
    "The following code creates and trains a voting classifier in Scikit-Learn, composed of three diverse\n",
    "classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X=make_moons(10000)\n",
    "from sklearn.model_selection import train_test_split\n",
    "#From tuple to numpy array of moon data\n",
    "data=X[0]\n",
    "labels=X[1]\n",
    "#data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.20, random_state=42)\n",
    "#X_train=X_trtest[]\n",
    "# what does the random state param a part from the random sampling methode notice? \n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.895\n",
      "RandomForestClassifier 1.0\n",
      "SVC 1.0\n",
      "VotingClassifier 0.999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'    \\nLogisticRegression 0.864\\nRandomForestClassifier 0.872\\nSVC 0.888\\nVotingClassifier 0.896\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability=True)\n",
    "voting_clf = VotingClassifier(\n",
    "estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "voting='soft'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "'''    \n",
    "LogisticRegression 0.864\n",
    "RandomForestClassifier 0.872\n",
    "SVC 0.888\n",
    "VotingClassifier 0.896\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all classifiers are able to estimate **class probabilities** (i.e., they have a predict_proba() method),\n",
    "then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the\n",
    "individual classifiers. This is called **soft voting**.It often achieves higher performance than hard voting\n",
    "because it gives more weight to highly confident votes.\n",
    "\n",
    "All you need to do is replace **voting=\"hard\"** with voting=\"soft\" and ensure that all classifiers can estimate class probabilities. This is not the case of the **SVC** class by default, so you need to set its probability hyperparameter to True (this will make the SVC class use cross-validation to estimate class probabilities, slowing down training, and it will add\n",
    "a predict_proba() method). If you modify the preceding code to use soft voting, you will find that the\n",
    "voting classifier achieves over 91% accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting\n",
    "\n",
    "Another approach is to use the same training algorithm for every predictor, but to train them on different\n",
    "**random subsets** of the training set. When **sampling** is performed **with replacement**, this method is called\n",
    "**bagging** (short for **bootstrap aggregating**). When sampling is performed without replacement, it is called **pasting**\n",
    "\n",
    "In other words, both **bagging** and **pasting** allow training instances to be **sampled several times** across\n",
    "multiple **predictors**, but only **bagging** allows training instances to be **sampled several times for the same\n",
    "predictor**.\n",
    "\n",
    "\n",
    "predictors can all be trained in parallel, via different **CPU cores** or even different **servers**. Similarly, predictions can be made in parallel. This is one of the reasons why bagging and pasting are such popular methods: they scale very well\n",
    "\n",
    "\n",
    "\n",
    "#### Bagging and Pasting in Scikit-Learn\n",
    "Scikit-Learn offers a simple API for both bagging and pasting with the **BaggingClassifier** class (or BaggingRegressor for regression). The following code trains an ensemble of **500 Decision Tree classifiers**, each trained on 100 training instances **randomly sampled** from the training set **with replacement** (this is an example of bagging, but if you want to use pasting instead, just set **bootstrap=False**). The **n_jobs** parameter tells Scikit-Learn the number of **CPU cores** to use for training and predictions (–1 tells Scikit-Learn to use all available cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(), n_estimators=500,\n",
    "max_samples=100, bootstrap=True, n_jobs=-1\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging\n",
    "ends up with a slightly higher bias than pasting, but this also means that predictors end up being less\n",
    "correlated so the ensemble’s variance is reduced. Overall, bagging often results in better models, which\n",
    "explains why it is generally preferred. However, if you have spare time and CPU power you can use\n",
    "cross-validation to evaluate both bagging and pasting and select the one that works best\n",
    "\n",
    "\n",
    "#### Out-of-Bag Evaluation\n",
    "With bagging, some instances may be sampled several times for any given predictor, while others may not\n",
    "be sampled at all. By default a BaggingClassifier samples m training instances with replacement\n",
    "(bootstrap=True), where m is the size of the training set. This means that only about 63% of the training\n",
    "instances are sampled on average for each predictor.6 The remaining 37% of the training instances that are\n",
    "not sampled are called out-of-bag (oob) instances. Note that they are not the same 37% for all predictors.\n",
    "Since a predictor never sees the oob instances during training, it can be evaluated on these instances,\n",
    "without the need for a separate validation set or cross-validation. You can evaluate the ensemble itself by\n",
    "averaging out the oob evaluations of each predictor.\n",
    "In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to request an\n",
    "automatic oob evaluation after training. The following code demonstrates this. The resulting evaluation\n",
    "score is available through the oob_score_ variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99962499999999999"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(), n_estimators=500,\n",
    "bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_\n",
    "# it is possible to have he oob decision function for each instance : bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99850000000000005"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with cross validation we are not far from the result above\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Patches and Random Subspaces\n",
    "The **BaggingClassifier** class supports sampling the features as well. This is controlled by two hyperparameters: **max_features** and **bootstrap_features**. They work the same way as **max_samples** and **bootstrap**, but for **feature sampling** instead of **instance sampling**. Thus, each predictor will be trained on a **random subset** of the input features.\n",
    "This is particularly useful when you are dealing with **high-dimensional inputs** (such as images). Sampling both training instances and features is called the **Random Patches method**. Keeping all training instances (i.e., bootstrap=False and max_samples=1.0) but sampling features (i.e., bootstrap_features=True and/or max_features smaller than 1.0) is called the Random Subspaces method. Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "As we have discussed, a **Random Forest** is an ensemble of **Decision Trees**, generally trained via the **bagging method** (or sometimes pasting), typically with **max_samples** set to the size of the training set. Instead of building a **BaggingClassifier** and passing it a **DecisionTreeClassifier**, you can instead use the **RandomForestClassifier** class, which is more convenient and optimized for **Decision Trees** (similarly, there is a **RandomForestRegressor** class for **regression** tasks). \n",
    "The following code trains a **Random Forest classifier** with 500 trees (each limited to maximum **16 nodes**), using all available CPU cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a few exceptions, a **RandomForestClassifier** has all the hyperparameters of a DecisionTreeClassifier (to control how trees are grown), plus all the hyperparameters of a **BaggingClassifier** to control the ensemble itself. The **Random Forest** algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. This results in a greater tree diversity, which (once again) trades a higher bias for a lower variance, generally yielding an overall better model. \n",
    "\n",
    "The following BaggingClassifier is roughly equivalent to the previous **RandomForestClassifier**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra-Trees\n",
    "When you are growing a tree in a Random Forest, at each node only a random subset of the features is\n",
    "considered for splitting (as discussed earlier). It is possible to make trees even more random by also\n",
    "using random thresholds for each feature rather than searching for the best possible thresholds (like\n",
    "regular Decision Trees do).\n",
    "A forest of such extremely random trees is simply called an Extremely Randomized Trees ensemble12 (or\n",
    "Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees\n",
    "much faster to train than regular Random Forests since finding the best possible threshold for each feature\n",
    "at every node is one of the most time-consuming tasks of growing a tree.\n",
    "You can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier class. Its API is\n",
    "identical to the RandomForestClassifier class. Similarly, the ExtraTreesRegressor class has the\n",
    "same API as the RandomForestRegressor class.\n",
    "\n",
    "\n",
    "#### TIP\n",
    "It is hard to tell in advance whether a RandomForestClassifier will perform better or worse than an ExtraTreesClassifier.\n",
    "Generally, the only way to know is to try both and compare them using cross-validation (and tuning the hyperparameters using\n",
    "grid search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "Lastly, if you look at a single Decision Tree, important features are likely to appear closer to the root of the tree, while unimportant features will often appear closer to the leaves (or not at all). It is therefore possible to get an estimate of a feature’s importance by computing the average depth at which it appears across all trees in the forest. Scikit-Learn computes this automatically for every feature after training. You can access the result using the feature_importances_ variable. For example, the following code trains a RandomForestClassifier on the iris dataset  and outputs each feature’s importance. It seems that the most important features are the petal length (44%) and width (42%), while sepal length and width are rather unimportant in comparison (11% and 2%, respectively):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 7.97603502116\n",
      "sepal width (cm) 2.23496633708\n",
      "petal length (cm) 44.3933630567\n",
      "petal width (cm) 45.3956355851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if you train a Random Forest classifier on the MNIST dataset (introduced in Chapter 3) and\n",
    "plot each pixel’s importance\n",
    "\n",
    "Random Forests are very handy to get a quick understanding of what features actually matter, in particular\n",
    "if you need to perform feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "Boosting (originally called hypothesis boosting) refers to any **Ensemble method** that can combine several **weak learners** into a **strong learner**. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. There are many boosting methods available, but by far the most popular are **AdaBoost** (short for Adaptive Boosting) and **Gradient Boosting**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost\n",
    "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training\n",
    "instances that the predecessor underfitted. This results in new predictors focusing more and more on the\n",
    "hard cases. This is the technique used by AdaBoost.\n",
    "For example, to build an AdaBoost classifier, a first base classifier (such as a Decision Tree) is trained\n",
    "and used to make predictions on the training set. The relative weight of misclassified training instances is\n",
    "then increased. A second classifier is trained using the updated weights and again it makes predictions on\n",
    "the training set, weights are updated, and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classifier\n",
    "gets many instances wrong, so their weights get boosted. The second classifier therefore does a better job\n",
    "on these instances, and so on. The plot on the right represents the same sequence of predictors except that\n",
    "the learning rate is halved (i.e., the misclassified instance weights are boosted half as much at every\n",
    "iteration). As you can see, this sequential learning technique has some similarities with Gradient Descent,\n",
    "except that instead of tweaking a single predictor’s parameters to minimize a cost function, AdaBoost\n",
    "adds predictors to the ensemble, gradually making it better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code trains an AdaBoost classifier based on 200 Decision Stumps using Scikit-Learn’s\n",
    "AdaBoostClassifier class (as you might expect, there is also an AdaBoostRegressor class). A\n",
    "Decision Stump is a Decision Tree with max_depth=1 — in other words, a tree composed of a single\n",
    "decision node plus two leaf nodes. This is the default base estimator for the AdaBoostClassifier class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "          learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(\n",
    "DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "algorithm=\"SAMME.R\", learning_rate=0.5\n",
    ")\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of estimators or more strongly\n",
    "regularizing the base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "Another very popular Boosting algorithm is Gradient Boosting.17 Just like AdaBoost, Gradient Boosting\n",
    "works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However,\n",
    "instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the\n",
    "new predictor to the residual errors made by the previous predictor.\n",
    "Let’s go through a simple regression example using Decision Trees as the base predictors (of course\n",
    "Gradient Boosting also works great with regression tasks). This is called Gradient Tree Boosting, or\n",
    "Gradient Boosted Regression Trees (GBRT). First, let’s fit a DecisionTreeRegressor to the training set\n",
    "(for example, a noisy quadratic training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X_train, y_train)\n",
    "#Now train a second DecisionTreeRegressor on the residual errors made by the first predictor:\n",
    "y2 = y_train - tree_reg1.predict(X_train)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X_train, y2)\n",
    "#Then we train a third regressor on the residual errors made by the second predictor:\n",
    "y3 = y2 - tree_reg2.predict(X_train)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X_train, y3)\n",
    "#Now we have an ensemble containing three trees. It can make predictions on a new instance simply by\n",
    "#adding up the predictions of all the trees:\n",
    "#y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simpler way to train GBRT ensembles is to use Scikit-Learn’s GradientBoostingRegressor class.\n",
    "Much like the RandomForestRegressor class, it has hyperparameters to control the growth of Decision\n",
    "Trees (e.g., max_depth, min_samples_leaf, and so on), as well as hyperparameters to control the\n",
    "ensemble training, such as the number of trees (n_estimators). The following code creates the same\n",
    "ensemble as the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=1.0, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=3, presort='auto',\n",
       "             random_state=None, subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning_rate hyperparameter scales the contribution of each tree. If you set it to a low value, such\n",
    "as 0.1, you will need more trees in the ensemble to fit the training set, but the predictions will usually\n",
    "generalize better. This is a regularization technique called shrinkage. Figure 7-10 shows two GBRT\n",
    "ensembles trained with a low learning rate: the one on the left does not have enough trees to fit the\n",
    "training set, while the one on the right has too many trees and overfits the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the optimal number of trees, you can use early stopping (see Chapter 4). A simple way to\n",
    "implement this is to use the staged_predict() method: it returns an iterator over the predictions made\n",
    "by the ensemble at each stage of training (with one tree, two trees, etc.). The following code trains a\n",
    "GBRT ensemble with 120 trees, then measures the validation error at each stage of training to find the\n",
    "optimal number of trees, and finally trains another GBRT ensemble using the optimal number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=119,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to implement early stopping by actually stopping training early (instead of training a\n",
    "large number of trees first and then looking back to find the optimal number). You can do so by setting\n",
    "warm_start=True, which makes Scikit-Learn keep existing trees when the fit() method is called,\n",
    "allowing incremental training. The following code stops training when the validation error does not\n",
    "improve for five iterations in a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break # early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GradientBoostingRegressor class also supports a subsample hyperparameter, which specifies\n",
    "the fraction of training instances to be used for training each tree. For example, if subsample=0.25, then\n",
    "each tree is trained on 25% of the training instances, selected randomly. As you can probably guess by\n",
    "now, this trades a higher bias for a lower variance. It also speeds up training considerably. This technique\n",
    "is called Stochastic Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to use Gradient Boosting with other cost functions. This is controlled by the loss hyperparameter (see Scikit-\n",
    "Learn’s documentation for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "The last Ensemble method we will discuss in this chapter is called stacking (short for stacked\n",
    "generalization).18 It is based on a simple idea: instead of using trivial functions (such as hard voting) to\n",
    "aggregate the predictions of all predictors in an ensemble, why don’t we train a model to perform this\n",
    "aggregation? Figure 7-12 shows such an ensemble performing a regression task on a new instance. Each\n",
    "of the bottom three predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor\n",
    "(called a blender, or a meta learner) takes these predictions as inputs and makes the final prediction\n",
    "(3.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
