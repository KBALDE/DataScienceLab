{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Decision Tree \n",
    "Like **SVMs**, **Decision Trees** are versatile Machine Learning algorithms that can perform both\n",
    "**classification** and **regression** tasks, and even multioutput tasks. They are very powerful algorithms,\n",
    "capable of fitting complex datasets.\n",
    "\n",
    "**Decision Trees** are also the fundamental components of **Random Forests**, which are among the most powerful Machine Learning algorithms available today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Visualizing a Decision Tree\n",
    "To understand Decision Trees, let’s just build one and take a look at how it makes predictions. The\n",
    "following code trains a DecisionTreeClassifier on the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the trained Decision Tree by first using the export_graphviz() method to output a\n",
    "graph definition file called iris_tree.dot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(\n",
    "tree_clf,\n",
    "out_file=\"iris_tree.dot\",\n",
    "feature_names=iris.feature_names[2:],\n",
    "class_names=iris.target_names,\n",
    "rounded=True,\n",
    "filled=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can convert this .dot file to a variety of formats such as PDF or PNG using the dot commandline\n",
    "tool from the graphviz package.1 This command line converts the .dot file to a .png image file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "(graph,) = pydot.graph_from_dot_file('iris_tree.dot')\n",
    "#graph.write_png('iris_tree.png')\n",
    "# It is like I need to find a right way to set a path for graphviz for it to find dot.exe\n",
    "# C:\\Program Files (x86)\\Graphviz2.38\\bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from subprocess import check_call\n",
    "#check_call(['dot','-Tpng','iris_tree.dot','-o','iris_tree.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the many qualities of Decision Trees is that they require very little data preparation. In particular, they don’t require\n",
    "feature scaling or centering at all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "A node’s samples attribute counts how **many training instances** it applies to. For example, 100 training\n",
    "instances have a petal length greater than 2.45 cm (depth 1, right), among which 54 have a petal width\n",
    "smaller than 1.75 cm (depth 2, left). A node’s value attribute tells you how many training instances of\n",
    "each class this node applies to: for example, the bottom-right node applies to 0 Iris-Setosa, 1 Iris-\n",
    "Versicolor, and 45 Iris-Virginica.\n",
    "Finally, a node’s **gini** attribute measures its **impurity**: a node is “pure” (gini=0) if all training instances it applies to belong to the same class. For example, since the depth-1 left node applies only to Iris-Setosa training instances, it is pure and its gini score is 0. shows how the training algorithm computes the gini score Gi of the ith node. For example, the depth-2 left node has a **gini score** equal to **1 – (0/54)2 – (49/54)2 – (5/54)2 ≈ 0.168**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Class Probabilities\n",
    "A Decision Tree can also estimate the probability that an instance belongs to a particular class k: first it\n",
    "traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of\n",
    "class k in this node. For example, suppose you have found a flower whose petals are 5 cm long and 1.5\n",
    "cm wide. The corresponding leaf node is the depth-2 left node, so the Decision Tree should output the\n",
    "following probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54), and 9.3% for Iris-\n",
    "Virginica (5/54). And of course if you ask it to predict the class, it should output Iris-Versicolor (class 1)\n",
    "since it has the highest probability. Let’s check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])\n",
    "#array([[ 0. , 0.90740741, 0.09259259]])\n",
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CART Training Algorithm\n",
    "Scikit-Learn uses the Classification And Regression Tree (CART) algorithm to train Decision Trees (also\n",
    "called “growing” trees). The idea is really quite simple: the algorithm first splits the training set in two\n",
    "subsets using a single feature k and a threshold tk (e.g., “petal length ≤ 2.45 cm”). How does it choose k\n",
    "and tk? It searches for the pair (k, tk) that produces the purest subsets (weighted by their size).\n",
    "\n",
    "\n",
    "Once it has successfully split the training set in two, it splits the subsets using the same logic, then the subsubsets\n",
    "and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the max_depth hyperparameter), or if it cannot find a split that will reduce impurity. A few other hyperparameters (described in a moment) control additional stopping conditions (min_samples_split, min_samples_leaf, min_weight_fraction_leaf, and max_leaf_nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning\n",
    "As you can see, the CART algorithm is a greedy algorithm: it greedily searches for an optimum split at the top level, then\n",
    "repeats the process at each level. It does not check whether or not the split will lead to the lowest possible impurity several levels\n",
    "down. A greedy algorithm often produces a reasonably good solution, but it is not guaranteed to be the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Impurity or Entropy?\n",
    "By default, the Gini impurity measure is used, but you can select the entropy impurity measure instead by\n",
    "setting the criterion hyperparameter to \"entropy\". The concept of entropy originated in\n",
    "thermodynamics as a measure of molecular disorder: entropy approaches zero when molecules are still\n",
    "and well ordered. It later spread to a wide variety of domains, including Shannon’s information theory,\n",
    "where it measures the average information content of a message:4 entropy is zero when all messages are\n",
    "identical. In Machine Learning, it is frequently used as an impurity measure: a set’s entropy is zero when\n",
    "it contains instances of only one class.\n",
    "\n",
    "Hi = - sum (Pik * log(Pik)) \n",
    "\n",
    "So should you use Gini impurity or entropy? The truth is, most of the time it does not make a big\n",
    "difference: they lead to similar trees. Gini impurity is slightly faster to compute, so it is a good default.\n",
    "However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the\n",
    "tree, while entropy tends to produce slightly more balanced trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Hyperparameters\n",
    "Decision Trees make very few assumptions about the training data (as opposed to linear models, which\n",
    "obviously assume that the data is linear, for example). If left unconstrained, the tree structure will adapt\n",
    "itself to the training data, fitting it very closely, and most likely overfitting it. Such a model is often called\n",
    "a nonparametric model, not because it does not have any parameters (it often has a lot) but because the\n",
    "number of parameters is not determined prior to training, so the model structure is free to stick closely to\n",
    "the data. In contrast, a parametric model such as a linear model has a predetermined number of\n",
    "parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of\n",
    "underfitting).\n",
    "To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom during training. As\n",
    "you know by now, this is called regularization. The regularization hyperparameters depend on the\n",
    "algorithm used, but generally you can at least restrict the maximum depth of the Decision Tree. In Scikit-\n",
    "Learn, this is controlled by the max_depth hyperparameter (the default value is None, which means\n",
    "unlimited). Reducing max_depth will regularize the model and thus reduce the risk of overfitting.\n",
    "The DecisionTreeClassifier class has a few other parameters that similarly restrict the shape of the\n",
    "Decision Tree: min_samples_split (the minimum number of samples a node must have before it can be\n",
    "split), min_samples_leaf (the minimum number of samples a leaf node must have),\n",
    "min_weight_fraction_leaf (same as min_samples_leaf but expressed as a fraction of the total\n",
    "number of weighted instances), max_leaf_nodes (maximum number of leaf nodes), and max_features\n",
    "(maximum number of features that are evaluated for splitting at each node). Increasing min_*\n",
    "hyperparameters or reducing max_* hyperparameters will regularize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "Other algorithms work by first training the Decision Tree without restrictions, then pruning (deleting) unnecessary nodes. A node\n",
    "whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically significant.\n",
    "Standard statistical tests, such as the χ2 test, are used to estimate the probability that the improvement is purely the result of\n",
    "chance (which is called the null hypothesis). If this probability, called the p-value, is higher than a given threshold (typically 5%,\n",
    "controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted. The pruning continues until\n",
    "all unnecessary nodes have been pruned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "Decision Trees are also capable of performing regression tasks. Let’s build a regression tree using Scikit-\n",
    "Learn’s DecisionTreeRegressor class, training it on a noisy quadratic dataset with max_depth=2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
